{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3WXXWAZgToM"
      },
      "outputs": [],
      "source": [
        "# References:\n",
        "# https://towardsdatascience.com/text-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e - Why both encoder\n",
        "# and decoder for text summarization\n",
        "# https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb - Majority of the source code\n",
        "# https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452 - Inference algo\n",
        "# https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34 - Visual explanation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIcX1tJ-hCix"
      },
      "outputs": [],
      "source": [
        "# importing libraries and setting device type\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPFVs8L_4MH_",
        "outputId": "d0cb62a1-733f-40e2-e32e-bf4983092c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/.shortcut-targets-by-id/1kYtz2nPcpky_CSkXMRMwo7J9aQIQpSEz/AML_final_project\n"
          ]
        }
      ],
      "source": [
        "# mounting drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "%cd '/content/drive/MyDrive/AML_final_project'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvWfRBE82C2u"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0FMuRLohhJ_"
      },
      "outputs": [],
      "source": [
        "# defining class for multi-head attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "        \n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "        \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "        \n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3dnlXOBhtzY"
      },
      "outputs": [],
      "source": [
        "# defining feedforward layer\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6np-eURiQcD"
      },
      "outputs": [],
      "source": [
        "# defining positional encoding\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIKxzx1_ilIT"
      },
      "outputs": [],
      "source": [
        "# defining the encoder class\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O0Pmm8njOPk"
      },
      "outputs": [],
      "source": [
        "# defining the decoder class\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQDfgepnkMJV"
      },
      "outputs": [],
      "source": [
        "# merging it all together\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "      src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "      src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "      tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "      enc_output = src_embedded\n",
        "      for enc_layer in self.encoder_layers:\n",
        "          enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "      dec_output = tgt_embedded\n",
        "      for dec_layer in self.decoder_layers:\n",
        "          dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "      output = self.fc(dec_output)\n",
        "      return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heSHM2SF2LAq"
      },
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SkcQVWhm4c-G",
        "outputId": "6f55e98a-ee0d-43ac-81ee-46746d38b159"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Headline  \\\n",
              "0  4 ex-bank officials booked for cheating bank o...   \n",
              "1     Supreme Court to go paperless in 6 months: CJI   \n",
              "2  At least 3 killed, 30 injured in blast in Sylh...   \n",
              "3  Why has Reliance been barred from trading in f...   \n",
              "4  Was stopped from entering my own studio at Tim...   \n",
              "\n",
              "                                               Short                 Source   \\\n",
              "0  The CBI on Saturday booked four former officia...  The New Indian Express   \n",
              "1  Chief Justice JS Khehar has said the Supreme C...                 Outlook   \n",
              "2  At least three people were killed, including a...         Hindustan Times   \n",
              "3  Mukesh Ambani-led Reliance Industries (RIL) wa...                Livemint   \n",
              "4  TV news anchor Arnab Goswami has said he was t...                 YouTube   \n",
              "\n",
              "      Time  Publish Date  \n",
              "0  09:25:00   2017-03-26  \n",
              "1  22:18:00   2017-03-25  \n",
              "2  23:39:00   2017-03-25  \n",
              "3  23:08:00   2017-03-25  \n",
              "4  23:24:00   2017-03-25  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae062762-67b6-44c1-ae44-716040a2b82f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "      <th>Source</th>\n",
              "      <th>Time</th>\n",
              "      <th>Publish Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "      <td>The New Indian Express</td>\n",
              "      <td>09:25:00</td>\n",
              "      <td>2017-03-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "      <td>Outlook</td>\n",
              "      <td>22:18:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "      <td>Hindustan Times</td>\n",
              "      <td>23:39:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "      <td>Livemint</td>\n",
              "      <td>23:08:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "      <td>YouTube</td>\n",
              "      <td>23:24:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae062762-67b6-44c1-ae44-716040a2b82f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae062762-67b6-44c1-ae44-716040a2b82f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae062762-67b6-44c1-ae44-716040a2b82f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# reading in full data\n",
        "\n",
        "full_data = pd.read_excel('Inshorts_Cleaned_Data.xlsx')\n",
        "full_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb8KEQmj6iHr",
        "outputId": "21e01364-77e8-4054-a3d7-ac173d9c3314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "## preprocessing\n",
        "\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "nltk.download('stopwords')    # downloading stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# function to carry out preprocessing\n",
        "def preprocess(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text = text.split() # convert have'nt -> have not\n",
        "    for i in range(len(text)):\n",
        "        word = text[i]\n",
        "        if word in contraction_mapping:\n",
        "            text[i] = contraction_mapping[word]\n",
        "    text = \" \".join(text)\n",
        "    text = text.split()\n",
        "    newtext = []\n",
        "    for word in text:\n",
        "        if word not in stop_words:\n",
        "            newtext.append(word)\n",
        "    text = \" \".join(newtext)\n",
        "    text = text.replace(\"'s\",'') # convert your's -> your\n",
        "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
        "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
        "    text = re.sub(r'\\.',' . ',text)\n",
        "    return text\n",
        "\n",
        "# executing preprocessing\n",
        "full_data['Headline'] = full_data['Headline'].apply(lambda x:preprocess(x))\n",
        "full_data['Short'] = full_data['Short'].apply(lambda x:preprocess(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting into train, validation and test\n",
        "\n",
        "train_df, validate_df, test_df = \\\n",
        "              np.split(full_data.sample(frac=1, random_state=42), \n",
        "                       [int(.6*len(full_data)), int(.8*len(full_data))])\n",
        "              \n",
        "# resetting index for newly created df's\n",
        "train_df = train_df.reset_index()\n",
        "validate_df = validate_df.reset_index()\n",
        "test_df = test_df.reset_index()\n"
      ],
      "metadata": {
        "id": "Bn9LvDfQiBkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg4Wyed279Qh"
      },
      "outputs": [],
      "source": [
        "# defining classes and functions to prep the data\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "def readLangs(text, summary, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "    \n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(summary)\n",
        "        output_lang = Lang(text)\n",
        "    else:\n",
        "        input_lang = Lang(text)\n",
        "        output_lang = Lang(summary)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    # print(\"Counted words:\")\n",
        "    # print(input_lang.name, input_lang.n_words)\n",
        "    # print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair, input_lang, output_lang):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "# setting max sequence lengths for padding purposes (IMP: These are arbitrary and need to be estimated using percentile analysis)\n",
        "max_src_length = 90 \n",
        "max_tgt_length = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training data prep"
      ],
      "metadata": {
        "id": "Oq7JrSX-iOHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_df['Short']\n",
        "y = train_df['Headline']\n",
        "train_input_lang, train_output_lang, train_pairs = prepareData(x, y ,False)\n",
        "print(random.choice(train_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr8z5iLziNdX",
        "outputId": "ef657b57-05df-417c-b1c6-ba02396d1491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 33062 sentence pairs\n",
            "Counting words...\n",
            "['coming home leg champions league round 16 20 lead barcelona defeated arsenal 31 wednesday advance quarterfinals tournament 51 aggregate score .  barcelona forward trio neymar luis surez lionel messi scored one each mohamed elnenny sole scorer gunners . ', 'barcelona advance champions league quarters']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1PWttuPgC-P",
        "outputId": "bcfa0029-3e00-4b30-d6a1-9c5273e84fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of source data tensor matrix is torch.Size([33062, 90])\n",
            "Shape of target data tensor matrix is torch.Size([33062, 15])\n"
          ]
        }
      ],
      "source": [
        "# creating training pairs in tensor form\n",
        "tensor_pairs = [tensorsFromPair(pair, train_input_lang, train_output_lang) for pair in train_pairs]\n",
        "\n",
        "# creating source data after padding\n",
        "src_data = torch.empty((1, max_src_length), dtype=int, device=device)\n",
        "for pair in tensor_pairs:\n",
        "  src_tensor = pair[0]\n",
        "  pad_len = max_src_length - src_tensor.shape[0]\n",
        "  padded_src = F.pad(input=src_tensor, pad=(0, 0, 0, pad_len),\n",
        "                     mode='constant', value=0).reshape(1, max_src_length)\n",
        "  src_data = torch.cat((src_data, padded_src))                \n",
        "\n",
        "src_data = src_data[1:,:]   # final version of source data tensors\n",
        "\n",
        "# creating target data after padding\n",
        "tgt_data = torch.empty((1, max_tgt_length), dtype=int, device=device)\n",
        "for pair in tensor_pairs:\n",
        "  tgt_tensor = pair[1]\n",
        "  pad_len = max_tgt_length - tgt_tensor.shape[0]\n",
        "  padded_tgt = F.pad(input=tgt_tensor, pad=(0, 0, 0, pad_len),\n",
        "                     mode='constant', value=0).reshape(1, max_tgt_length)\n",
        "  tgt_data = torch.cat((tgt_data, padded_tgt))                \n",
        "\n",
        "tgt_data = tgt_data[1:,:]   # final version of target data tensors\n",
        "\n",
        "# cross-checking shapes\n",
        "print(\"Shape of source data tensor matrix is\", src_data.shape)\n",
        "print(\"Shape of target data tensor matrix is\", tgt_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating custom dataset for training data\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_data[idx]\n",
        "        tgt = self.tgt_data[idx]      \n",
        "        return src, tgt\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "# instantiating\n",
        "train_iter = NewsDataset(src_data, tgt_data)\n"
      ],
      "metadata": {
        "id": "LEOOlkyoBsd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataloader\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# instantiate dataloader objects for training data\n",
        "train_dataloader = DataLoader(train_iter, batch_size=batch_size)\n",
        "\n",
        "# testing dataloader\n",
        "for idx, (src, tgt) in enumerate(train_dataloader):\n",
        "    print(idx, src.shape, tgt.shape)\n",
        "    if idx == 4: \n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDQ0cL_R81e_",
        "outputId": "f27888c3-5eef-4d53-835a-f84cfb53a28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "1 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "2 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "3 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "4 torch.Size([16, 90]) torch.Size([16, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation data prep"
      ],
      "metadata": {
        "id": "KULotyuomfwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = validate_df['Short']\n",
        "y = validate_df['Headline']\n",
        "val_input_lang, val_output_lang, val_pairs = prepareData(x, y ,False)\n",
        "print(random.choice(val_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6fa1839-4d94-48cf-e8d4-091c1f22a3e6",
        "id": "7RgICs5cmfwa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 11021 sentence pairs\n",
            "Counting words...\n",
            "['swedish automaker volvo39s custombuilt truck 39the iron knight39 become world39s fastest truck covering kilometre 21 . 29 seconds .  truck achieved average speed 169 kmph powered ishift dual clutch transmission .  also became world39s fastest truck cover 500metre distance 13 . 71 seconds . ', 'world39s fastest truck covers 1 kilometre 21 . 2 secs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11f15d1-71b8-4928-c4c3-c6e3f1b89f84",
        "id": "1uMan1limfwa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of source data tensor matrix is torch.Size([11021, 90])\n",
            "Shape of target data tensor matrix is torch.Size([11021, 15])\n"
          ]
        }
      ],
      "source": [
        "# creating validation pairs in tensor form\n",
        "tensor_pairs = [tensorsFromPair(pair, val_input_lang, val_output_lang) for pair in val_pairs]\n",
        "\n",
        "# creating source data after padding\n",
        "src_data = torch.empty((1, max_src_length), dtype=int, device=device)\n",
        "for pair in tensor_pairs:\n",
        "  src_tensor = pair[0]\n",
        "  pad_len = max_src_length - src_tensor.shape[0]\n",
        "  padded_src = F.pad(input=src_tensor, pad=(0, 0, 0, pad_len),\n",
        "                     mode='constant', value=0).reshape(1, max_src_length)\n",
        "  src_data = torch.cat((src_data, padded_src))                \n",
        "\n",
        "src_data = src_data[1:,:]   # final version of source data tensors\n",
        "\n",
        "# creating target data after padding\n",
        "tgt_data = torch.empty((1, max_tgt_length), dtype=int, device=device)\n",
        "for pair in tensor_pairs:\n",
        "  tgt_tensor = pair[1]\n",
        "  pad_len = max_tgt_length - tgt_tensor.shape[0]\n",
        "  padded_tgt = F.pad(input=tgt_tensor, pad=(0, 0, 0, pad_len),\n",
        "                     mode='constant', value=0).reshape(1, max_tgt_length)\n",
        "  tgt_data = torch.cat((tgt_data, padded_tgt))                \n",
        "\n",
        "tgt_data = tgt_data[1:,:]   # final version of target data tensors\n",
        "\n",
        "# cross-checking shapes\n",
        "print(\"Shape of source data tensor matrix is\", src_data.shape)\n",
        "print(\"Shape of target data tensor matrix is\", tgt_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating custom dataset for validation data\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_data[idx]\n",
        "        tgt = self.tgt_data[idx]      \n",
        "        return src, tgt\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "# instantiating\n",
        "val_iter = NewsDataset(src_data, tgt_data)\n"
      ],
      "metadata": {
        "id": "fqPI8u45mfwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataloader\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# instantiate dataloader objects for validation data\n",
        "val_dataloader = DataLoader(val_iter, batch_size=batch_size)\n",
        "\n",
        "# testing dataloader\n",
        "for idx, (src, tgt) in enumerate(val_dataloader):\n",
        "    print(idx, src.shape, tgt.shape)\n",
        "    if idx == 4: \n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d110a5-96a2-4937-f90f-84af61aab80b",
        "id": "qXY-h9ZJmfwa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "1 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "2 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "3 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "4 torch.Size([16, 90]) torch.Size([16, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test data prep"
      ],
      "metadata": {
        "id": "GYHJFriYnNXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = test_df['Short']\n",
        "y = test_df['Headline']\n",
        "test_input_lang, test_output_lang, test_pairs = prepareData(x, y ,False)\n",
        "print(random.choice(test_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04aabdd6-c9da-4856-b1d3-95bd601c704f",
        "id": "q-zbaG0VnNXn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 11021 sentence pairs\n",
            "Counting words...\n",
            "['reality television star kim kardashian denied reports new sex tape featuring leaked online saying 34it39s new it39s old tape . 34 report claimed video showed woman 34appearing kim34 bra showing cleavage man .  notably kim39s sex tape boyfriend leaked 2007 . ', 'kim kardashian denies rumours new sex tape leaked']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b749e7d-902c-44ed-e1a5-316c4c9fbc23",
        "id": "jqUzZVQqnNXn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of source data tensor matrix is torch.Size([11021, 90])\n",
            "Shape of target data tensor matrix is torch.Size([11021, 15])\n"
          ]
        }
      ],
      "source": [
        "# creating test pairs in tensor form\n",
        "tensor_pairs = [tensorsFromPair(pair, test_input_lang, test_output_lang) for pair in test_pairs]\n",
        "\n",
        "# creating source data after padding\n",
        "src_data = torch.empty((1, max_src_length), dtype=int, device=device)\n",
        "for pair in tensor_pairs:\n",
        "  src_tensor = pair[0]\n",
        "  pad_len = max_src_length - src_tensor.shape[0]\n",
        "  padded_src = F.pad(input=src_tensor, pad=(0, 0, 0, pad_len),\n",
        "                     mode='constant', value=0).reshape(1, max_src_length)\n",
        "  src_data = torch.cat((src_data, padded_src))                \n",
        "\n",
        "src_data = src_data[1:,:]   # final version of source data tensors\n",
        "\n",
        "# creating target data after padding\n",
        "tgt_data = torch.empty((1, max_tgt_length), dtype=int, device=device)\n",
        "for pair in tensor_pairs:\n",
        "  tgt_tensor = pair[1]\n",
        "  pad_len = max_tgt_length - tgt_tensor.shape[0]\n",
        "  padded_tgt = F.pad(input=tgt_tensor, pad=(0, 0, 0, pad_len),\n",
        "                     mode='constant', value=0).reshape(1, max_tgt_length)\n",
        "  tgt_data = torch.cat((tgt_data, padded_tgt))                \n",
        "\n",
        "tgt_data = tgt_data[1:,:]   # final version of target data tensors\n",
        "\n",
        "# cross-checking shapes\n",
        "print(\"Shape of source data tensor matrix is\", src_data.shape)\n",
        "print(\"Shape of target data tensor matrix is\", tgt_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating custom dataset for test data\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_data[idx]\n",
        "        tgt = self.tgt_data[idx]      \n",
        "        return src, tgt\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "# instantiating\n",
        "test_iter = NewsDataset(src_data, tgt_data)\n"
      ],
      "metadata": {
        "id": "oPztd0RqnNXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataloader\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# instantiate dataloader objects for test data\n",
        "test_dataloader = DataLoader(test_iter, batch_size=batch_size)\n",
        "\n",
        "# testing dataloader\n",
        "for idx, (src, tgt) in enumerate(test_dataloader):\n",
        "    print(idx, src.shape, tgt.shape)\n",
        "    if idx == 4: \n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a45a25-1ce3-4344-c24d-f9035967d00e",
        "id": "gGou93IAnNXo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "1 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "2 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "3 torch.Size([16, 90]) torch.Size([16, 15])\n",
            "4 torch.Size([16, 90]) torch.Size([16, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "BIWrvyDnnO82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation functions for validation and test data\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    '''\n",
        "    Evaluate the model on the given data.\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "    it = iter(data_loader)\n",
        "    total_count = 0 # Number of target words seen\n",
        "    total_loss = 0 # Loss over all target words\n",
        "    with torch.no_grad():\n",
        "        # No gradients need to be maintained during evaluation\n",
        "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "        for i, batch in enumerate(it):\n",
        "            ''' Do the following:\n",
        "                - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
        "                  the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
        "                  https://pytorch.org/docs/stable/notes/cuda.html.\n",
        "                - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
        "                  the current batch. \n",
        "                - Call forward propagation to get output and final hidden state vector.\n",
        "                - Compute the cross entropy loss\n",
        "                - The loss_fn computes the average loss per target word in the batch.  Count the number of target\n",
        "                  words in the batch (it is usually the same, except for the last batch), and use it to track the \n",
        "                  total count (of target words) and total loss see so far over all batches.\n",
        "            '''\n",
        "            src, tgt = batch\n",
        "            output = transformer(src, tgt[:, :-1])\n",
        "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt[:, 1:].contiguous().view(-1))\n",
        "            total_count += np.multiply(*src.size())\n",
        "            total_loss += loss.item()*np.multiply(*src.size())\n",
        "\n",
        "    loss = total_loss / total_count\n",
        "    model.train()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "aR4s_LbWpIEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO5OID2fDHmk"
      },
      "outputs": [],
      "source": [
        "# instantiating parameters and transformer model instance\n",
        "\n",
        "src_vocab_size = train_input_lang.n_words\n",
        "tgt_vocab_size = train_output_lang.n_words\n",
        "d_model = 512   # dimension of embedding\n",
        "num_heads = 8   # number of attention heads\n",
        "num_layers = 6  # number of encoder and decoder layers\n",
        "d_ff = 2048     # dimension of feedforward layer input\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, \n",
        "                          num_layers, d_ff, max_src_length, dropout)\n",
        "transformer = transformer.to(device)    # getting model to run on CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9cyN7GvoFlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db93e432-b8fe-4bed-dad0-a1b92d1b6852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######### EPOCH 1 ##########\n",
            "At iteration 200 the TRAINING loss is 7.882.\n",
            "At iteration 400 the TRAINING loss is 8.093.\n",
            "At iteration 500 the VALIDATION loss is 8.801.\n",
            "Best model changed at iteration 500\n",
            "At iteration 600 the TRAINING loss is 8.018.\n",
            "At iteration 800 the TRAINING loss is 7.313.\n",
            "At iteration 1000 the TRAINING loss is 8.134.\n",
            "At iteration 1000 the VALIDATION loss is 8.845.\n",
            "At iteration 1200 the TRAINING loss is 7.764.\n",
            "At iteration 1400 the TRAINING loss is 7.635.\n",
            "At iteration 1500 the VALIDATION loss is 8.945.\n",
            "At iteration 1600 the TRAINING loss is 8.160.\n",
            "At iteration 1800 the TRAINING loss is 7.602.\n",
            "At iteration 2000 the TRAINING loss is 7.734.\n",
            "At iteration 2000 the VALIDATION loss is 8.996.\n",
            "\n",
            "At end of Epoch: 1, Training Loss: 7.173739433288574\n",
            "\n",
            "\n",
            "######### EPOCH 2 ##########\n",
            "At iteration 200 the TRAINING loss is 6.882.\n",
            "At iteration 400 the TRAINING loss is 7.375.\n",
            "At iteration 500 the VALIDATION loss is 9.251.\n",
            "At iteration 600 the TRAINING loss is 7.556.\n",
            "At iteration 800 the TRAINING loss is 6.705.\n",
            "At iteration 1000 the TRAINING loss is 7.402.\n",
            "At iteration 1000 the VALIDATION loss is 9.494.\n",
            "At iteration 1200 the TRAINING loss is 7.275.\n",
            "At iteration 1400 the TRAINING loss is 7.126.\n",
            "At iteration 1500 the VALIDATION loss is 9.588.\n",
            "At iteration 1600 the TRAINING loss is 7.574.\n",
            "At iteration 1800 the TRAINING loss is 7.000.\n",
            "At iteration 2000 the TRAINING loss is 7.200.\n",
            "At iteration 2000 the VALIDATION loss is 9.101.\n",
            "\n",
            "At end of Epoch: 2, Training Loss: 6.608097553253174\n",
            "\n",
            "\n",
            "######### EPOCH 3 ##########\n",
            "At iteration 200 the TRAINING loss is 6.286.\n",
            "At iteration 400 the TRAINING loss is 6.942.\n",
            "At iteration 500 the VALIDATION loss is 9.340.\n",
            "At iteration 600 the TRAINING loss is 7.116.\n",
            "At iteration 800 the TRAINING loss is 6.232.\n",
            "At iteration 1000 the TRAINING loss is 6.979.\n",
            "At iteration 1000 the VALIDATION loss is 9.641.\n",
            "At iteration 1200 the TRAINING loss is 6.874.\n",
            "At iteration 1400 the TRAINING loss is 6.689.\n",
            "At iteration 1500 the VALIDATION loss is 9.814.\n",
            "At iteration 1600 the TRAINING loss is 7.119.\n",
            "At iteration 1800 the TRAINING loss is 6.331.\n",
            "At iteration 2000 the TRAINING loss is 6.738.\n",
            "At iteration 2000 the VALIDATION loss is 9.269.\n",
            "\n",
            "At end of Epoch: 3, Training Loss: 5.850221633911133\n",
            "\n",
            "\n",
            "######### EPOCH 4 ##########\n",
            "At iteration 200 the TRAINING loss is 5.933.\n",
            "At iteration 400 the TRAINING loss is 6.557.\n",
            "At iteration 500 the VALIDATION loss is 9.461.\n",
            "At iteration 600 the TRAINING loss is 6.719.\n",
            "At iteration 800 the TRAINING loss is 5.676.\n",
            "At iteration 1000 the TRAINING loss is 6.635.\n",
            "At iteration 1000 the VALIDATION loss is 9.786.\n",
            "At iteration 1200 the TRAINING loss is 6.476.\n",
            "At iteration 1400 the TRAINING loss is 6.223.\n",
            "At iteration 1500 the VALIDATION loss is 9.983.\n",
            "At iteration 1600 the TRAINING loss is 6.621.\n",
            "At iteration 1800 the TRAINING loss is 5.871.\n",
            "At iteration 2000 the TRAINING loss is 6.309.\n",
            "At iteration 2000 the VALIDATION loss is 9.456.\n",
            "\n",
            "At end of Epoch: 4, Training Loss: 5.323160648345947\n",
            "\n",
            "\n",
            "######### EPOCH 5 ##########\n",
            "At iteration 200 the TRAINING loss is 5.677.\n",
            "At iteration 400 the TRAINING loss is 6.060.\n",
            "At iteration 500 the VALIDATION loss is 9.529.\n",
            "At iteration 600 the TRAINING loss is 6.422.\n",
            "At iteration 800 the TRAINING loss is 5.329.\n",
            "At iteration 1000 the TRAINING loss is 6.363.\n",
            "At iteration 1000 the VALIDATION loss is 9.960.\n",
            "At iteration 1200 the TRAINING loss is 5.973.\n",
            "At iteration 1400 the TRAINING loss is 5.793.\n",
            "At iteration 1500 the VALIDATION loss is 10.183.\n",
            "At iteration 1600 the TRAINING loss is 6.190.\n",
            "At iteration 1800 the TRAINING loss is 5.339.\n",
            "At iteration 2000 the TRAINING loss is 6.006.\n",
            "At iteration 2000 the VALIDATION loss is 9.787.\n",
            "\n",
            "At end of Epoch: 5, Training Loss: 4.866009712219238\n",
            "\n",
            "\n",
            "######### EPOCH 6 ##########\n",
            "At iteration 200 the TRAINING loss is 5.325.\n",
            "At iteration 400 the TRAINING loss is 5.820.\n",
            "At iteration 500 the VALIDATION loss is 9.543.\n",
            "At iteration 600 the TRAINING loss is 6.072.\n",
            "At iteration 800 the TRAINING loss is 4.958.\n",
            "At iteration 1000 the TRAINING loss is 6.075.\n",
            "At iteration 1000 the VALIDATION loss is 10.246.\n",
            "At iteration 1200 the TRAINING loss is 5.694.\n",
            "At iteration 1400 the TRAINING loss is 5.507.\n",
            "At iteration 1500 the VALIDATION loss is 10.335.\n",
            "At iteration 1600 the TRAINING loss is 5.864.\n",
            "At iteration 1800 the TRAINING loss is 4.984.\n",
            "At iteration 2000 the TRAINING loss is 5.636.\n",
            "At iteration 2000 the VALIDATION loss is 9.957.\n",
            "\n",
            "At end of Epoch: 6, Training Loss: 4.090523719787598\n",
            "\n",
            "\n",
            "######### EPOCH 7 ##########\n",
            "At iteration 200 the TRAINING loss is 5.060.\n",
            "At iteration 400 the TRAINING loss is 5.622.\n",
            "At iteration 500 the VALIDATION loss is 9.786.\n",
            "At iteration 600 the TRAINING loss is 5.808.\n",
            "At iteration 800 the TRAINING loss is 4.667.\n",
            "At iteration 1000 the TRAINING loss is 5.940.\n",
            "At iteration 1000 the VALIDATION loss is 10.260.\n",
            "At iteration 1200 the TRAINING loss is 5.388.\n",
            "At iteration 1400 the TRAINING loss is 5.181.\n",
            "At iteration 1500 the VALIDATION loss is 10.402.\n",
            "At iteration 1600 the TRAINING loss is 5.609.\n",
            "At iteration 1800 the TRAINING loss is 4.606.\n",
            "At iteration 2000 the TRAINING loss is 5.396.\n",
            "At iteration 2000 the VALIDATION loss is 10.007.\n",
            "\n",
            "At end of Epoch: 7, Training Loss: 3.8103115558624268\n",
            "\n",
            "\n",
            "######### EPOCH 8 ##########\n",
            "At iteration 200 the TRAINING loss is 4.739.\n",
            "At iteration 400 the TRAINING loss is 5.222.\n",
            "At iteration 500 the VALIDATION loss is 9.927.\n",
            "At iteration 600 the TRAINING loss is 5.649.\n",
            "At iteration 800 the TRAINING loss is 4.375.\n",
            "At iteration 1000 the TRAINING loss is 5.405.\n",
            "At iteration 1000 the VALIDATION loss is 10.556.\n",
            "At iteration 1200 the TRAINING loss is 5.136.\n",
            "At iteration 1400 the TRAINING loss is 4.829.\n",
            "At iteration 1500 the VALIDATION loss is 10.766.\n",
            "At iteration 1600 the TRAINING loss is 5.440.\n",
            "At iteration 1800 the TRAINING loss is 4.355.\n",
            "At iteration 2000 the TRAINING loss is 5.003.\n",
            "At iteration 2000 the VALIDATION loss is 10.464.\n",
            "\n",
            "At end of Epoch: 8, Training Loss: 3.1442089080810547\n",
            "\n",
            "\n",
            "######### EPOCH 9 ##########\n",
            "At iteration 200 the TRAINING loss is 4.480.\n",
            "At iteration 400 the TRAINING loss is 4.988.\n",
            "At iteration 500 the VALIDATION loss is 10.329.\n",
            "At iteration 600 the TRAINING loss is 5.054.\n",
            "At iteration 800 the TRAINING loss is 3.934.\n",
            "At iteration 1000 the TRAINING loss is 5.107.\n",
            "At iteration 1000 the VALIDATION loss is 10.751.\n",
            "At iteration 1200 the TRAINING loss is 4.949.\n",
            "At iteration 1400 the TRAINING loss is 4.624.\n",
            "At iteration 1500 the VALIDATION loss is 11.000.\n",
            "At iteration 1600 the TRAINING loss is 5.113.\n",
            "At iteration 1800 the TRAINING loss is 4.052.\n",
            "At iteration 2000 the TRAINING loss is 4.839.\n",
            "At iteration 2000 the VALIDATION loss is 10.588.\n",
            "\n",
            "At end of Epoch: 9, Training Loss: 2.9482200145721436\n",
            "\n",
            "\n",
            "######### EPOCH 10 ##########\n",
            "At iteration 200 the TRAINING loss is 4.238.\n",
            "At iteration 400 the TRAINING loss is 4.699.\n",
            "At iteration 500 the VALIDATION loss is 10.464.\n",
            "At iteration 600 the TRAINING loss is 4.746.\n",
            "At iteration 800 the TRAINING loss is 3.914.\n",
            "At iteration 1000 the TRAINING loss is 5.091.\n",
            "At iteration 1000 the VALIDATION loss is 11.065.\n",
            "At iteration 1200 the TRAINING loss is 4.673.\n",
            "At iteration 1400 the TRAINING loss is 4.455.\n",
            "At iteration 1500 the VALIDATION loss is 11.148.\n",
            "At iteration 1600 the TRAINING loss is 4.814.\n",
            "At iteration 1800 the TRAINING loss is 3.820.\n",
            "At iteration 2000 the TRAINING loss is 4.576.\n",
            "At iteration 2000 the VALIDATION loss is 10.943.\n",
            "\n",
            "At end of Epoch: 10, Training Loss: 2.5027377605438232\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "train_log_interval = 200\n",
        "val_log_interval = 500\n",
        "\n",
        "val_losses = []\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(10):\n",
        "  transformer.train()\n",
        "  print(\"######### EPOCH {} ##########\".format(epoch + 1))\n",
        "  for idx, (src, tgt) in enumerate(train_dataloader):\n",
        "    transformer.zero_grad()\n",
        "    output = transformer(src, tgt[:, :-1])    # why removing the last column in target?\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (idx + 1) % train_log_interval == 0:\n",
        "      print(f'At iteration {idx + 1} the TRAINING loss is {loss:.3f}.')\n",
        "\n",
        "    if (idx + 1) % val_log_interval == 0:\n",
        "      val_loss = evaluate(transformer, val_dataloader)\n",
        "      val_losses.append(val_loss)\n",
        "      print(f'At iteration {idx + 1} the VALIDATION loss is {val_loss:.3f}.')\n",
        "      if val_loss <= min(val_losses):\n",
        "          print(\"Best model changed at iteration\", idx + 1)\n",
        "          best_model = type(transformer)(src_vocab_size, tgt_vocab_size, d_model, num_heads, \n",
        "                        num_layers, d_ff, max_src_length, dropout) # get a new instance\n",
        "          best_model.to(device=device)\n",
        "          best_model.load_state_dict(transformer.state_dict()) # copy weights\n",
        "  \n",
        "  print()\n",
        "  print(f\"At end of Epoch: {epoch+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "  print()\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the best models to disk\n",
        "\n",
        "torch.save(best_model.state_dict(), \"/content/drive/MyDrive/AML_final_project/best_model_weights/best_transformer_val_model.pt\")\n",
        "torch.save(transformer.state_dict(), \"/content/drive/MyDrive/AML_final_project/best_model_weights/best_transformer_train_model.pt\")"
      ],
      "metadata": {
        "id": "kx-JiwvcwhJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "WuVIJXNV09PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the trained model from disk\n",
        "\n",
        "trained_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, \n",
        "                          num_layers, d_ff, max_src_length, dropout)\n",
        "trained_model = trained_model.to(device)\n",
        "trained_model.load_state_dict(torch.load(\"/content/drive/MyDrive/AML_final_project/best_model_weights/best_transformer_train_model.pt\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "BsVB6KvmcLbn",
        "outputId": "20b436dc-b860-4b3e-9341-22daa9ccfd7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b346b580baa1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading the trained model from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m trained_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, \n\u001b[0m\u001b[1;32m      4\u001b[0m                           num_layers, d_ff, max_src_length, dropout)\n\u001b[1;32m      5\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Transformer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6JbZboLqyfC"
      },
      "outputs": [],
      "source": [
        "# defining functions for visual model evaluation\n",
        "\n",
        "def evaluate(model, input_lang, output_lang, sentence, max_len = 15):\n",
        "    '''\n",
        "    Same as training procedure above, except there are no targets.\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # prepping input sentence\n",
        "      input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "      input_tensor = input_tensor.reshape(1, input_tensor.size()[0])\n",
        "\n",
        "      decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "      decoded_words = []\n",
        "\n",
        "      for di in range(max_len):\n",
        "        output = model(input_tensor, decoder_input)\n",
        "\n",
        "        # extracting vector corresponding to last word\n",
        "        last = output.squeeze(0)[-1, :].unsqueeze(0)\n",
        "\n",
        "        # doing softmax\n",
        "        m = nn.Softmax(dim=1)\n",
        "        output_probs = m(last)\n",
        "\n",
        "        # extracting index with highest probability\n",
        "        idx = torch.argmax(last, dim=1)\n",
        "        print(\"index is\", idx)\n",
        "        if idx.item() == EOS_token:\n",
        "            decoded_words.append('<EOS>')\n",
        "            break\n",
        "        else:\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "\n",
        "        decoder_input = torch.cat((decoder_input, idx.unsqueeze(0)), 1).to(device)\n",
        "\n",
        "      return decoder_input, decoded_words\n",
        "\n",
        "\n",
        "def evaluateRandomly(model, input_lang, output_lang, pairs, n=5):\n",
        "    '''\n",
        "    Evaluate random sentences from the dataset and print out the text, actual\n",
        "    summary and predicted summary to make some subjective quality judgements.\n",
        "    '''\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('Text:', pair[0])\n",
        "        print('Actual summary:', pair[1])\n",
        "        decoder_input, decoded_words = evaluate(model, \n",
        "                                                input_lang,\n",
        "                                                output_lang,\n",
        "                                                pair[0])\n",
        "        output_sentence = ' '.join(decoded_words)\n",
        "        print('Predicted summary:', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visual inspection on training data predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "gXXuVa0T2LlA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeqRNqnQATHz",
        "outputId": "51cbea16-f24f-49fb-e600-297a6264c47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: islamic state  acknowledged death 39jihadi john39 british maskedmilitant appearing videos depicting beheadings western hostages site intelligence group reported tuesday .  published eulogy englishlanguage magazine 39dabiq39 report added .  us military november claimed killed 39jihadi john39 dronestrike . \n",
            "Actual summary: acknowledges death jihadi john report\n",
            "Predicted summary: app helps man killed syria <EOS>\n",
            "\n",
            "Text: couple us state oklahoma clemma sterling elmore photoshoot inspired 2004 movie 39the notebook39 celebrate 57 years togetherness .  setting shoot complete vintage babyblue truck 1940sstyle clothing handwritten love notes .  photoshoot organised granddaughter done stacy welch christ . \n",
            "Actual summary: couple 39the notebook39 inspired photoshoot\n",
            "Predicted summary: reelect house new year jail <EOS>\n",
            "\n",
            "Text: actress aishwarya rai bachchan best actress award role 39sarbjit39 international film festival awards australia .  based life sarabjit singh indian prisoner pakistan film aishwarya portraying sister 39dalbir kaur39 .  also nominated best actress category 39sarbjit39 filmfare awards . \n",
            "Actual summary: aishwarya wins best actress sarbjit int39l film fest\n",
            "Predicted summary: virat kohli named best film awards 2017 <EOS>\n",
            "\n",
            "Text: global education training firm aptech39s shares surged 6 hit fresh 52week high company reported strong quarterly earnings .  company reported consolidated net profit 7 . 17 crore quarter ended september 30 2 . 75 crore yearago period thereby posting nearly threefold jump profits . \n",
            "Actual summary: aptech shares hit 52week high robust q2 results\n",
            "Predicted summary: q1 net profit rises 25 . 2 . 2 cr <EOS>\n",
            "\n",
            "Text: us electronics firm griffin made magnetic cable charge new macbook pro laptops announced apple using 39magsafe39 technology .  40  cable used device usbc port including google pixel lineup .  apple used magnetic charging technology decade announcing discontinuation week . \n",
            "Actual summary: firm makes 40 cable add 39magsafe39 new macbooks\n",
            "Predicted summary: develops handheld device travels 110 years <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(trained_model, train_input_lang, train_output_lang, train_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visual inspection on test data predictions"
      ],
      "metadata": {
        "id": "UbE2wEU82XBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9cpd34hmS-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271092f3-eceb-4af1-a081-9a8074b05a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: nita ambani indias first female member international olympic committee became countrys first woman award medals olympic prize distribution ceremony .  ambani gave away medals winners womens 400metre freestyle swimming event monday .  ambani elected ioc august 4 remain member till turns 70 . \n",
            "Actual summary: nita 1st indian woman award olympic medals\n",
            "Predicted summary: university home wounded dimple grass <EOS>\n",
            "\n",
            "Text: ban highdenomination currency led mismatch cash supply figures recent report suggesting people withdrawn 60000 crore actual currency circulation january 13 .  total currency circulation reported 9 . 1 lakh crore public already withdrawn close 9 . 7 lakh crore . \n",
            "Actual summary: cash withdrawn 60000 cr circulation report\n",
            "Predicted summary: startups ipo spa irans olav exhusband <EOS>\n",
            "\n",
            "Text: congress vice president rahul gandhi tuesday said pressure government decided rollback tax employees39 provident fund  withdrawals .  said 34whenever somebody oppressed wrongly victimized tend try help people .  felt salaried middleclass people hurt government . 34\n",
            "Actual summary: epf tax rollback pressure rahul\n",
            "Predicted summary: distributes kannada gandhi footballer praneeth <EOS>\n",
            "\n",
            "Text: market regulator sebi monday raised limit foreign portfolio investment government securities 1 . 48 lakh crore existing cap 1 . 44 lakh crore .  increased 1 . 52 lakh crore january .  sebi also increased investment limit longterm fpis  government securities 62000 crore . \n",
            "Actual summary: sebi increases fpi limit govt securities 4k cr\n",
            "Predicted summary: larger distributes maxwell crew39s sena <EOS>\n",
            "\n",
            "Text: 39gram art project39 creating rakhis seeds attached centres taken grow plant represents 39the blooming relationship brother sister39 .  rakhis cost 20 25 30 enclose information workforce involved growing cotton rakhis spinning yarn designing finished products . \n",
            "Actual summary: art initiative creates rakhi plant seed\n",
            "Predicted summary: counterpart swift39 covering <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(trained_model, test_input_lang, test_output_lang, test_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating BLEU score on test data predictions"
      ],
      "metadata": {
        "id": "0JAgGpDQ41GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvsXIrp95p3B",
        "outputId": "9c4bb4df-a9a0-4bbc-bf6a-0a009e489287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the necessary objects\n",
        "\n",
        "predicted_summaries = []\n",
        "actual_summaries = []\n",
        "\n",
        "# actual summaries\n",
        "for pair in test_pairs:\n",
        "  actual_summaries.append(pair[1])\n",
        "\n",
        "# predicted summaries\n",
        "for pair in test_pairs:\n",
        "  decoder_input, decoded_words = evaluate(trained_model, \n",
        "                                          test_input_lang,\n",
        "                                          test_output_lang,\n",
        "                                          pair[0])\n",
        "  output_sentence = ' '.join(decoded_words)\n",
        "  predicted_summaries.append(output_sentence)\n"
      ],
      "metadata": {
        "id": "MOLVcp0Y5zid",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e50d7fc-d6bc-498f-b27f-c4a441f8eb24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index is tensor([773], device='cuda:0')\n",
            "index is tensor([642], device='cuda:0')\n",
            "index is tensor([357], device='cuda:0')\n",
            "index is tensor([943], device='cuda:0')\n",
            "index is tensor([1432], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([789], device='cuda:0')\n",
            "index is tensor([642], device='cuda:0')\n",
            "index is tensor([694], device='cuda:0')\n",
            "index is tensor([2142], device='cuda:0')\n",
            "index is tensor([3789], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([1649], device='cuda:0')\n",
            "index is tensor([697], device='cuda:0')\n",
            "index is tensor([3546], device='cuda:0')\n",
            "index is tensor([1834], device='cuda:0')\n",
            "index is tensor([61], device='cuda:0')\n",
            "index is tensor([862], device='cuda:0')\n",
            "index is tensor([240], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([4205], device='cuda:0')\n",
            "index is tensor([530], device='cuda:0')\n",
            "index is tensor([107], device='cuda:0')\n",
            "index is tensor([397], device='cuda:0')\n",
            "index is tensor([1294], device='cuda:0')\n",
            "index is tensor([4527], device='cuda:0')\n",
            "index is tensor([558], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([5612], device='cuda:0')\n",
            "index is tensor([5613], device='cuda:0')\n",
            "index is tensor([1491], device='cuda:0')\n",
            "index is tensor([3271], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([9124], device='cuda:0')\n",
            "index is tensor([3279], device='cuda:0')\n",
            "index is tensor([3307], device='cuda:0')\n",
            "index is tensor([187], device='cuda:0')\n",
            "index is tensor([4793], device='cuda:0')\n",
            "index is tensor([2767], device='cuda:0')\n",
            "index is tensor([2768], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([1730], device='cuda:0')\n",
            "index is tensor([7746], device='cuda:0')\n",
            "index is tensor([10642], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([201], device='cuda:0')\n",
            "index is tensor([1293], device='cuda:0')\n",
            "index is tensor([777], device='cuda:0')\n",
            "index is tensor([1884], device='cuda:0')\n",
            "index is tensor([1100], device='cuda:0')\n",
            "index is tensor([1722], device='cuda:0')\n",
            "index is tensor([295], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([823], device='cuda:0')\n",
            "index is tensor([56], device='cuda:0')\n",
            "index is tensor([824], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([5612], device='cuda:0')\n",
            "index is tensor([5613], device='cuda:0')\n",
            "index is tensor([1491], device='cuda:0')\n",
            "index is tensor([3271], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([1011], device='cuda:0')\n",
            "index is tensor([13051], device='cuda:0')\n",
            "index is tensor([1722], device='cuda:0')\n",
            "index is tensor([257], device='cuda:0')\n",
            "index is tensor([2133], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([3307], device='cuda:0')\n",
            "index is tensor([1053], device='cuda:0')\n",
            "index is tensor([397], device='cuda:0')\n",
            "index is tensor([45], device='cuda:0')\n",
            "index is tensor([46], device='cuda:0')\n",
            "index is tensor([240], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([108], device='cuda:0')\n",
            "index is tensor([1706], device='cuda:0')\n",
            "index is tensor([1706], device='cuda:0')\n",
            "index is tensor([1706], device='cuda:0')\n",
            "index is tensor([1094], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([4491], device='cuda:0')\n",
            "index is tensor([2767], device='cuda:0')\n",
            "index is tensor([2768], device='cuda:0')\n",
            "index is tensor([397], device='cuda:0')\n",
            "index is tensor([1843], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([3307], device='cuda:0')\n",
            "index is tensor([1024], device='cuda:0')\n",
            "index is tensor([42], device='cuda:0')\n",
            "index is tensor([672], device='cuda:0')\n",
            "index is tensor([271], device='cuda:0')\n",
            "index is tensor([45], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([295], device='cuda:0')\n",
            "index is tensor([621], device='cuda:0')\n",
            "index is tensor([79], device='cuda:0')\n",
            "index is tensor([1227], device='cuda:0')\n",
            "index is tensor([1012], device='cuda:0')\n",
            "index is tensor([295], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([133], device='cuda:0')\n",
            "index is tensor([397], device='cuda:0')\n",
            "index is tensor([2438], device='cuda:0')\n",
            "index is tensor([139], device='cuda:0')\n",
            "index is tensor([293], device='cuda:0')\n",
            "index is tensor([140], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([6257], device='cuda:0')\n",
            "index is tensor([799], device='cuda:0')\n",
            "index is tensor([1375], device='cuda:0')\n",
            "index is tensor([8780], device='cuda:0')\n",
            "index is tensor([7587], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([4491], device='cuda:0')\n",
            "index is tensor([2767], device='cuda:0')\n",
            "index is tensor([2768], device='cuda:0')\n",
            "index is tensor([3447], device='cuda:0')\n",
            "index is tensor([649], device='cuda:0')\n",
            "index is tensor([3447], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([1209], device='cuda:0')\n",
            "index is tensor([1278], device='cuda:0')\n",
            "index is tensor([985], device='cuda:0')\n",
            "index is tensor([8841], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([1094], device='cuda:0')\n",
            "index is tensor([3307], device='cuda:0')\n",
            "index is tensor([187], device='cuda:0')\n",
            "index is tensor([950], device='cuda:0')\n",
            "index is tensor([2608], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([6483], device='cuda:0')\n",
            "index is tensor([1195], device='cuda:0')\n",
            "index is tensor([5218], device='cuda:0')\n",
            "index is tensor([1173], device='cuda:0')\n",
            "index is tensor([397], device='cuda:0')\n",
            "index is tensor([1294], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([6257], device='cuda:0')\n",
            "index is tensor([3307], device='cuda:0')\n",
            "index is tensor([1053], device='cuda:0')\n",
            "index is tensor([1893], device='cuda:0')\n",
            "index is tensor([9181], device='cuda:0')\n",
            "index is tensor([1094], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([1810], device='cuda:0')\n",
            "index is tensor([530], device='cuda:0')\n",
            "index is tensor([1340], device='cuda:0')\n",
            "index is tensor([148], device='cuda:0')\n",
            "index is tensor([443], device='cuda:0')\n",
            "index is tensor([1346], device='cuda:0')\n",
            "index is tensor([3795], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([185], device='cuda:0')\n",
            "index is tensor([2859], device='cuda:0')\n",
            "index is tensor([530], device='cuda:0')\n",
            "index is tensor([107], device='cuda:0')\n",
            "index is tensor([3415], device='cuda:0')\n",
            "index is tensor([1], device='cuda:0')\n",
            "index is tensor([5612], device='cuda:0')\n",
            "index is tensor([5613], device='cuda:0')\n",
            "index is tensor([26773], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-5b89c29ecdfc>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# predicted summaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   decoder_input, decoded_words = evaluate(trained_model, \n\u001b[0m\u001b[1;32m     13\u001b[0m                                           \u001b[0mtest_input_lang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                           \u001b[0mtest_output_lang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-5124c5bc4efc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, input_lang, output_lang, sentence, max_len)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mdecoded_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 26773"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the score\n",
        "\n",
        "import evaluate\n",
        "\n",
        "bleu = evaluate.load(\"google_bleu\")\n",
        "total_bleu_score = 0\n",
        "\n",
        "for prediction, actual in zip(predicted_summaries, actual_summaries):\n",
        "    total_bleu_score = total_bleu_score + bleu.compute(predictions=[prediction],\n",
        "                                                       references=[actual])['google_bleu']\n",
        "\n",
        "avg_bleu_score = total_bleu_score/len(predicted_summaries)\n",
        "print(\"The average BLEU score is\", avg_bleu_score)"
      ],
      "metadata": {
        "id": "H6KSbnEY46Em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77cdeaa7-5f3d-4f6f-8007-83f92e5f4a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average BLEU score is 0.006912580596791122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvj4PRY9xyl2",
        "outputId": "3f3230e4-a43e-4e05-e6ea-607560061bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['praneeth larger forgotten planet 20 <EOS>',\n",
              " 'talks give free crew39s kohinoor <EOS>',\n",
              " 'mushkil pact haters obamamichelle39s bombing scanning <EOS>',\n",
              " 'university dimple gandhi footballer dutchman <EOS>',\n",
              " 'venues 1 govt profit ae <EOS>',\n",
              " 'quarterfinal dimple members gandhi footballer larger <EOS>',\n",
              " 'undeclared jasprit stock bunting nationals tyson <EOS>',\n",
              " '32 depot netanyahu39s given playing film law offers <EOS>',\n",
              " 'startups cancer fb trailer irans <EOS>',\n",
              " 'larger listed tendulkar39s gandhi jackman39s <EOS>',\n",
              " 'university dimple smartphones h1b bse power <EOS>',\n",
              " 'kohinoor kerala gandhi spider centre bipasha <EOS>',\n",
              " 'delayed gandhi spider fade clean energy <EOS>',\n",
              " 'removed gandhi foreign jawan zurich <EOS>',\n",
              " 'university university netanyahu39s paid reveals pok apologise 2919 <EOS>',\n",
              " 'suspended scanning dimple russia basketball <EOS>',\n",
              " 'rajya criticism39 rising feature cr sasikala rising <EOS>',\n",
              " 'praneeth larger nonstop stage whatever hurdles <EOS>']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5hn6H7M0mn-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}